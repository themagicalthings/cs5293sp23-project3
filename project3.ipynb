{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2342813f",
   "metadata": {},
   "source": [
    "## Student Name: Vamsi Thokala\n",
    "## Student Email: Vamsi.thokala-1@ou.edu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84e6ab65",
   "metadata": {},
   "source": [
    "# Project 3: The Smart City Slicker\n",
    "\n",
    "Imagine you are a stakeholder in a rising Smart City and want to know more about themes and concepts about existing smart cities. You also want to know where does your smart city place among others. In this project, you will perform \n",
    "exploratory data analysis, often shortened to EDA, to examine a data from the [2015 Smart City Challenge](https://www.transportation.gov/smartcity) to find facts about the data and communicating those facts through text analysis and visualizations.\n",
    "\n",
    "In order to explore the data and visualize it, some modifications might need to be made to the data along the way. This is often referred to as data preprocessing or cleaning.\n",
    "Though data preprocessing is technically different from EDA, EDA often exposes problems with the data that need to be fixed in order to continue exploring.\n",
    "Because of this tight coupling, you have to clean the data as necessary to help understand the data.\n",
    "\n",
    "In this project, you will apply your knowledge about data cleaning, machine learning, visualizations, and databases to explore smart city applications.\n",
    "\n",
    "**Part 1** of the notebook will explore and clean the data. \\\n",
    "**Part 2** will take the results of the preprocessed data to create models and visualizations.\n",
    "\n",
    "Empty cells are code cells. \n",
    "Cells denoted with [Your Answer Here] are markdown cells.\n",
    "Edit and add as many cells as needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e8dcba1",
   "metadata": {},
   "source": [
    "Output file for this notebook is shown as a table for display purposes. Note: The city name can be Norman, OK or OK Norman.\n",
    "\n",
    "| city | raw text | clean text | clusterid | topicids | summary | keywords|\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "|Norman, OK | Test, test , and testing. | test test test | 0 | T1, T2| test | test |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd47ce",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The Dataset: 2015 Smart City Challenge Applicants (non-finalist).\n",
    "In this project you will use the applicant's PDFs as a dataset.\n",
    "The dataset is from the U.S Department of Transportation Smart City Challenge.\n",
    "\n",
    "On the website page for the data, you can find some basic information about the challenge. This is an interesting dataset. Think of the questions that you might be able to answer! A few could be:\n",
    "\n",
    "1. Can I identify frequently occurring words that could be removed during data preprocessing?\n",
    "2. Where are the applicants from?\n",
    "3. Are there multiple entries for the same city in different applicantions?\n",
    "4. What are the major themes and concepts from the smart city applicants?\n",
    "\n",
    "Let's load the data!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aace966",
   "metadata": {},
   "source": [
    "## Loading and Handling files\n",
    "\n",
    "Load data from `smartcity/`. \n",
    "\n",
    "To extract the data from the pdf files, use the [pypdf.pdf.PdfFileReader](https://pypdf.readthedocs.io/en/stable/index.html) class.\n",
    "It will allow you to extract pages and pdf files and add them to a data structure (dataframe, list, dictionary, etc).\n",
    "To install the module, use the command `pipenv install pypdf`.\n",
    "You only need to handle PDF files, handling docx is not necessary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31745abb",
   "metadata": {},
   "source": [
    "## Cleaning Up PDFs\n",
    "\n",
    "One of the more frustrating aspects of PDF is loading the data into a readable format. The first order of business will be to preprocess the data. To start, you can use code provided by Text Analytics with Python, [Chapter 3](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch03%20-%20Processing%20and%20Understanding%20Text/Ch03a%20-%20Text%20Wrangling.ipynb): [contractions.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/contractions.py) (Pages 136-137), and [text_normalizer.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/text_normalizer.py) (Pages 155-156). Feel free to download the scripts or add the code directly to the notebook (please note this code is performed on dataframes).\n",
    "\n",
    "In addition to the data cleaning provided by the textbook, you will need to:\n",
    "1. Consider removing terms that may effect clustering and topic modeling. Words to consider are cities, states, common words (smart, city, page, etc.). Keep in mind n-gram combinations are important; this can also be revisited later depending on your model's performance.\n",
    "2. Check the data to remove applicants that text was not processed correctly. Do not remove more than 15 cities from the data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8742fc4b",
   "metadata": {},
   "source": [
    "###Create a data structure to add the city name and raw text. You can choose to split the city name from the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a31f7e",
   "metadata": {},
   "source": [
    "#### Add the cleaned text to the structure you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4421d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyPDF2\n",
    "%pip install nltk\n",
    "%pip install matplotlib\n",
    "%pip install tabulate\n",
    "%pip install joblib\n",
    "%pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d2e8205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import PyPDF2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import joblib\n",
    "from tabulate import tabulate\n",
    "from PyPDF2 import PdfReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.getLogger(\"PyPDF2\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef0afb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vamsithokala/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vamsithokala/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e4905f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed city: OH Toledo, Issue: Text length (0) is below the threshold (20)\n",
      "Removed city: CA Moreno Valley, Issue: Text length (0) is below the threshold (20)\n",
      "Removed city: TX Lubbock, Issue: Text length (0) is below the threshold (20)\n",
      "Removed city: NV Reno, Issue: Text length (0) is below the threshold (20)\n",
      "Removed city: FL Tallahassee, Issue: Text length (0) is below the threshold (20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_pdf(file_path):\n",
    "    pdf = PdfReader(open(file_path, \"rb\"))\n",
    "    text = \" \".join([page.extract_text() for page in pdf.pages])\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text, extra_stopwords):\n",
    "    text = text.lower()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\") + extra_stopwords]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# Add your extra stopwords here\n",
    "extra_stopwords = [\"city\", \"state\", \"smart\", \"page\"]\n",
    "\n",
    "data = []\n",
    "min_text_length = 20  # Set a threshold for minimum text length\n",
    "removed_cities_count = 0\n",
    "removed_cities = []  # To store the removed cities and their issues\n",
    "directory = \"smartcity/\"\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        city = filename.split(\".\")[0]\n",
    "        raw_text = read_pdf(os.path.join(directory, filename))\n",
    "        clean_text = preprocess_text(raw_text, extra_stopwords)\n",
    "\n",
    "        # Check if the cleaned text is above the threshold and limit the number of removed cities\n",
    "        if len(clean_text) > min_text_length or removed_cities_count >= 15:\n",
    "            data.append([city, raw_text, clean_text])\n",
    "        else:\n",
    "            issue = f\"Text length ({len(clean_text)}) is below the threshold ({min_text_length})\"\n",
    "            removed_cities.append((city, issue))\n",
    "            removed_cities_count += 1\n",
    "\n",
    "# Display the removed cities and their issues\n",
    "for city, issue in removed_cities:\n",
    "    print(f\"Removed city: {city}, Issue: {issue}\")\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"city\", \"raw text\", \"clean text\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc947b",
   "metadata": {},
   "source": [
    "### Clean Up: Discussion\n",
    "Answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ba98d",
   "metadata": {},
   "source": [
    "#### Which Smart City applicants did you remove? What issues did you see with the documents?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffebf5a5",
   "metadata": {},
   "source": [
    "Removed city: OH Toledo, Issue: Text length (0) is below the threshold (20)\n",
    "Removed city: CA Moreno Valley, Issue: Text length (0) is below the threshold (20)\n",
    "Removed city: TX Lubbock, Issue: Text length (0) is below the threshold (20)\n",
    "Removed city: NV Reno, Issue: Text length (0) is below the threshold (20)\n",
    "Removed city: FL Tallahassee, Issue: Text length (0) is below the threshold (20)\n",
    "\n",
    "The text extraction process might have failed for these specific documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620ed74",
   "metadata": {},
   "source": [
    "#### Explain what additional text processing methods you used and why."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae42fc81",
   "metadata": {},
   "source": [
    "    preprocess_text() function takes in the raw text and a list of extra stopwords as arguments.\n",
    "    The text is converted to lowercase to maintain uniformity and to match words in the stopwords list.\n",
    "    The text is tokenized into words using the nltk.word_tokenize() function.\n",
    "    Only alphabetic words are retained, removing any numbers or special characters.\n",
    "    Stopwords, including the extra stopwords provided, are removed from the list of words.\n",
    "    Word lemmatization is performed using the WordNetLemmatizer() class from NLTK, which reduces words to their base form (lemma), improving the clustering process by grouping similar words together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15817355",
   "metadata": {},
   "source": [
    "#### Did you identify any potientally problematic words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad6082",
   "metadata": {},
   "source": [
    "[\"city\", \"state\", \"smart\", \"page\"]. These words are considered potentially problematic because they are common words that may appear frequently in the documents but do not provide any valuable information for clustering or topic modeling. Removing these words from the text helps to focus on more meaningful words and n-grams that better represent the content of the documents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1507fbe",
   "metadata": {},
   "source": [
    "## Experimenting with Clustering Models\n",
    "\n",
    "Now, you'll start to explore models to find the optimal clustering model. In this section, you'll explore [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), [Hierarchical](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html), and [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) clustering algorithms.\n",
    "Create these algorithms with k_clusters for K-means and Hierarchical.\n",
    "For each cell in the table provide the [Silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score), [Calinski and Harabasz score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score), and [Davies-Bouldin score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score).\n",
    "\n",
    "In each cell, create an array to store the values.\n",
    "For example, \n",
    "\n",
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means| [S,CH,DB]| [S,CH,DB] | [S,CH,DB] | [S,CH,DB] |\n",
    "|Hierarchical |[S,CH,DB]| [S,CH,DB]| [S,CH,DB] | [S,CH,DB]|\n",
    "|DBSCAN | X | X | X | [S,CH,DB] |\n",
    "\n",
    "\n",
    "\n",
    "### Optimality \n",
    "You will need to find the optimal k for K-means and Hierarchical algorithms.\n",
    "Find the optimality for k in the range 2 to 50.\n",
    "Provide the code used to generate the optimal k and provide justification for your approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2b033",
   "metadata": {},
   "source": [
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means|--|--|--|--|\n",
    "|Hierarchical |--|--|--|--|\n",
    "|DBSCAN | X | X | X | -- |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc8c7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'K-means': {2: [0.02770128465494533, 1.3158572530700587, 2.6636799594572254], 3: [0.01121621185016327, 1.2902488017744327, 4.3996535250427895], 4: [0.022162328255323067, 1.281477560379151, 1.744814589041059], 5: [0.02446750543661035, 1.283371997436961, 1.8026921615592957], 6: [-0.012313899118486302, 1.201426384937001, 1.1958985348125328], 7: [0.012274128628564094, 1.2226321056952096, 3.1172460205855788], 8: [0.011398520592593941, 1.208362992796128, 2.2499310096849805], 9: [0.0023772027586678726, 1.197555593073037, 2.698205182298869], 10: [0.008167119588534185, 1.2415145152388947, 1.9030219267197233], 11: [0.016183491993321378, 1.1980339529979003, 1.918036555761922], 12: [0.006970353078722772, 1.2055996420430035, 1.6782926867109855], 13: [-0.011306603213943726, 1.191558639123327, 1.3215828868016126], 14: [-0.0024697044251542556, 1.2053905594266479, 2.161152066837832], 15: [-0.0005339231983738511, 1.1747747092204457, 1.2884685877567226], 16: [-0.004127668833803753, 1.1788036718029997, 1.244288966462146], 17: [-0.0006384783645140319, 1.1835227710365483, 2.145431640729921], 18: [0.006174832248429246, 1.186459029117488, 1.4268983014425782], 19: [0.0059004682700941075, 1.176690479348127, 1.1310772596001797], 20: [0.012858594589329905, 1.1839835909313767, 1.645351403798585], 21: [0.007806122962618105, 1.1926888695032234, 1.3049410398932397], 22: [0.0036657466174302607, 1.1794469025109244, 1.255310585057427], 23: [0.001197493017866856, 1.192178297487029, 1.2577424750093475], 24: [0.012163091974011282, 1.1785423474994245, 1.2651801418314998], 25: [0.010093235784755763, 1.1913650175843231, 1.234535264564359], 26: [-0.001773458293858388, 1.176969318855806, 1.0178829492600083], 27: [0.004307938862718498, 1.1888178575506256, 1.3470254939159092], 28: [0.00728375670841092, 1.1848659687357566, 1.1709169551611958], 29: [0.006118136025524006, 1.176251497672217, 1.1532371166102784], 30: [0.00795917925093851, 1.176895623110726, 1.1423724505345834], 31: [0.00430982976090112, 1.1932451972486773, 1.0277485598144809], 32: [-0.0007594400769051195, 1.1836097233571568, 1.1216132113277693], 33: [0.008579929739569163, 1.1920591588587939, 1.1084479245616974], 34: [0.010382083690482076, 1.1915402598254792, 1.099641340977054], 35: [0.014463743026793416, 1.1954486765764865, 1.1036524335140312], 36: [0.010520396628776862, 1.2040251339858206, 1.0903694639686006], 37: [0.008057797846684252, 1.2028254966377612, 0.9969414501886344], 38: [0.012307955767832193, 1.1987763154155782, 1.0673569321382437], 39: [0.00911925852768937, 1.2105263749806219, 0.9861890565755564], 40: [0.010996214043522648, 1.210849409147125, 1.0335978508415675], 41: [0.013100157360615879, 1.214721953359723, 1.038805809954226], 42: [0.011714731069591332, 1.21939563117421, 1.0182526513239036], 43: [0.012347590263079819, 1.231510340831987, 1.0065167304838165], 44: [0.011907652137505931, 1.233648780062343, 0.9572946381382045], 45: [0.013044412870406533, 1.2395553746705827, 0.9901124893060697], 46: [0.01249560096231745, 1.2394645126140926, 0.9474235690946498], 47: [0.013588154764352111, 1.246120663919554, 0.9725690863838905], 48: [0.013728679261237251, 1.2583664298439168, 0.930302814077292], 49: [0.01361403795704368, 1.2562092643344416, 0.952978514988691], 50: [0.014632145325430264, 1.276606837592429, 0.9127657567488052]}, 'Hierarchical': {2: [0.021006084108672093, 1.5321814958798805, 1.719457487748704], 3: [0.014126635040206122, 1.5203890046905975, 4.801925056853235], 4: [0.014646991062395533, 1.4605830502108341, 4.036983061475767], 5: [0.016225595113254547, 1.4267730655478463, 3.774547711584701], 6: [0.017940168781849064, 1.3962899771701278, 3.1858614818919766], 7: [0.019384460942545225, 1.3639873324555318, 3.010336425717402], 8: [-0.006279092075898476, 1.3406500056970523, 2.868998704098003], 9: [-0.005884003672039498, 1.321875637659671, 2.8383503675345896], 10: [-0.0048904054222075385, 1.3063783317627498, 2.7744101313011025], 11: [-0.0044358904747642935, 1.2921700471966648, 2.551810984615958], 12: [-0.004036057163264712, 1.2810442046124177, 2.534949329584229], 13: [-0.0034395566516638295, 1.2709692289283998, 2.386038740207133], 14: [-0.0031240141225379048, 1.262510766872486, 2.339338816259835], 15: [-0.002477903975635484, 1.2555441456546965, 2.2199871464080183], 16: [-0.002414156018791968, 1.2495741384608343, 2.16824949922127], 17: [-0.0012683117470442206, 1.2444248118377153, 2.018001130181616], 18: [-0.0006814302710236174, 1.2402618351474752, 1.9317640541810341], 19: [5.842798361178512e-05, 1.236905076303797, 1.8118951768712233], 20: [0.000636753279079831, 1.2343233590113392, 1.7403036920275412], 21: [0.001029714895816802, 1.2316493248192828, 1.711508521178831], 22: [0.0010994674074837145, 1.2294091739267912, 1.6713818726059761], 23: [0.0017362493461688172, 1.2276162470084493, 1.597775364328484], 24: [0.002245103052367631, 1.226057301288821, 1.5412582808478772], 25: [0.00327914496417889, 1.225037473257702, 1.4640884110372197], 26: [0.0037685879374435077, 1.224570241155946, 1.417599590527052], 27: [0.005141263774103495, 1.2242443326755543, 1.3507127913004933], 28: [0.005459115667766301, 1.2237461254984967, 1.4660211032860726], 29: [0.0066452899822053955, 1.2236271999672053, 1.4029997075368563], 30: [0.007724572848330796, 1.2239534700459476, 1.3443599216194497], 31: [0.008704354352454043, 1.2247654591376853, 1.35975649613616], 32: [0.009310799177478615, 1.225452247992921, 1.3266494987091921], 33: [0.009879056934416292, 1.2260609121424717, 1.2916315273511396], 34: [0.010447809908601538, 1.2271319190998173, 1.2734370207002281], 35: [0.011403530663802031, 1.228547860361359, 1.2864345384948466], 36: [0.011921108847533508, 1.2301985227209782, 1.2419659786289674], 37: [0.012094723438627584, 1.23210861732986, 1.2189663505258568], 38: [0.012535290317869142, 1.2343265070350733, 1.18866560652325], 39: [0.0130116680325804, 1.2369881364165598, 1.1736832747755828], 40: [0.013555362596838275, 1.239890468538763, 1.1585184344101431], 41: [0.014040286144652931, 1.2434340286835344, 1.1237637774237692], 42: [0.014720849290066266, 1.2473038926362578, 1.1771999475961021], 43: [0.015360930584866785, 1.2505386419578433, 1.1429463403717792], 44: [0.015005070888305168, 1.254442342068859, 1.1116480334528709], 45: [0.015410086490655204, 1.2588606823086803, 1.0878884546597483], 46: [0.016136789396669484, 1.264014413915135, 1.0575506986449987], 47: [0.016597428157040173, 1.2699401899770766, 1.0310454976520735], 48: [0.017018484121513713, 1.2766746503405382, 0.999601191319018], 49: [0.017145614735529793, 1.2845457379856156, 0.9602079141750725], 50: [0.016637599050179645, 1.2933169269249714, 0.9316614857794094]}, 'DBSCAN': {'X': ['Not enough clusters', 'Not enough clusters', 'Not enough clusters']}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare the data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"clean text\"])\n",
    "X_dense = X.toarray()\n",
    "\n",
    "# Initialize variables\n",
    "k_values = list(range(2, 51))\n",
    "results = {\n",
    "    \"K-means\": {},\n",
    "    \"Hierarchical\": {},\n",
    "    \"DBSCAN\": {},\n",
    "}\n",
    "\n",
    "optimal_k = {\n",
    "    \"K-means\": {\"k\": 0, \"silhouette_score\": -1},\n",
    "    \"Hierarchical\": {\"k\": 0, \"silhouette_score\": -1},\n",
    "}\n",
    "\n",
    "# Evaluate clustering models\n",
    "for k in k_values:\n",
    "    # K-means clustering\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(X)\n",
    "    kmeans_silhouette = silhouette_score(X, kmeans_labels)\n",
    "    kmeans_calinski_harabasz = calinski_harabasz_score(X_dense, kmeans_labels)\n",
    "    kmeans_davies_bouldin = davies_bouldin_score(X_dense, kmeans_labels)\n",
    "    results[\"K-means\"][k] = [kmeans_silhouette, kmeans_calinski_harabasz, kmeans_davies_bouldin]\n",
    "\n",
    "    if kmeans_silhouette > optimal_k[\"K-means\"][\"silhouette_score\"]:\n",
    "        optimal_k[\"K-means\"][\"k\"] = k\n",
    "        optimal_k[\"K-means\"][\"silhouette_score\"] = kmeans_silhouette\n",
    "\n",
    "    # Hierarchical clustering\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=k)\n",
    "    hierarchical_labels = hierarchical.fit_predict(X_dense)\n",
    "    hierarchical_silhouette = silhouette_score(X, hierarchical_labels)\n",
    "    hierarchical_calinski_harabasz = calinski_harabasz_score(X_dense, hierarchical_labels)\n",
    "    hierarchical_davies_bouldin = davies_bouldin_score(X_dense, hierarchical_labels)\n",
    "    results[\"Hierarchical\"][k] = [hierarchical_silhouette, hierarchical_calinski_harabasz, hierarchical_davies_bouldin]\n",
    "\n",
    "    if hierarchical_silhouette > optimal_k[\"Hierarchical\"][\"silhouette_score\"]:\n",
    "        optimal_k[\"Hierarchical\"][\"k\"] = k\n",
    "        optimal_k[\"Hierarchical\"][\"silhouette_score\"] = hierarchical_silhouette\n",
    "\n",
    "# DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X)\n",
    "unique_labels = np.unique(dbscan_labels)\n",
    "\n",
    "if len(unique_labels) > 1:\n",
    "    dbscan_silhouette = silhouette_score(X, dbscan_labels)\n",
    "    dbscan_calinski_harabasz = calinski_harabasz_score(X_dense, dbscan_labels)\n",
    "    dbscan_davies_bouldin = davies_bouldin_score(X_dense, dbscan_labels)\n",
    "    results[\"DBSCAN\"] = {\"X\": [dbscan_silhouette, dbscan_calinski_harabasz, dbscan_davies_bouldin]}\n",
    "else:\n",
    "    results[\"DBSCAN\"] = {\"X\": [\"Not enough clusters\", \"Not enough clusters\", \"Not enough clusters\"]}\n",
    "\n",
    "\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d96d23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+---------+---------+---------+---------+---------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-------------+\n",
      "| Algorithm    | k = 2   | k = 3   | k = 4   | k = 5   | k = 6   | k = 7   | k = 8   | k = 9   | k = 10   | k = 11   | k = 12   | k = 13   | k = 14   | k = 15   | k = 16   | k = 17   | k = 18   | k = 19   | k = 20   | k = 21   | k = 22   | k = 23   | k = 24   | k = 25   | k = 26   | k = 27   | k = 28   | k = 29   | k = 30   | k = 31   | k = 32   | k = 33   | k = 34   | k = 35   | k = 36   | k = 37   | k = 38   | k = 39   | k = 40   | k = 41   | k = 42   | k = 43   | k = 44   | k = 45   | k = 46   | k = 47   | k = 48   | k = 49   | k = 50   | Optimal k   |\n",
      "+==============+=========+=========+=========+=========+=========+=========+=========+=========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+==========+=============+\n",
      "| K-means      | 0.0277  | 0.0112  | 0.0222  | 0.0245  | -0.0123 | 0.0123  | 0.0114  | 0.0024  | 0.0082   | 0.0162   | 0.007    | -0.0113  | -0.0025  | -0.0005  | -0.0041  | -0.0006  | 0.0062   | 0.0059   | 0.0129   | 0.0078   | 0.0037   | 0.0012   | 0.0122   | 0.0101   | -0.0018  | 0.0043   | 0.0073   | 0.0061   | 0.008    | 0.0043   | -0.0008  | 0.0086   | 0.0104   | 0.0145   | 0.0105   | 0.0081   | 0.0123   | 0.0091   | 0.011    | 0.0131   | 0.0117   | 0.0123   | 0.0119   | 0.013    | 0.0125   | 0.0136   | 0.0137   | 0.0136   | 0.0146   | 2           |\n",
      "+--------------+---------+---------+---------+---------+---------+---------+---------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-------------+\n",
      "| Hierarchical | 0.021   | 0.0141  | 0.0146  | 0.0162  | 0.0179  | 0.0194  | -0.0063 | -0.0059 | -0.0049  | -0.0044  | -0.004   | -0.0034  | -0.0031  | -0.0025  | -0.0024  | -0.0013  | -0.0007  | 0.0001   | 0.0006   | 0.001    | 0.0011   | 0.0017   | 0.0022   | 0.0033   | 0.0038   | 0.0051   | 0.0055   | 0.0066   | 0.0077   | 0.0087   | 0.0093   | 0.0099   | 0.0104   | 0.0114   | 0.0119   | 0.0121   | 0.0125   | 0.013    | 0.0136   | 0.014    | 0.0147   | 0.0154   | 0.015    | 0.0154   | 0.0161   | 0.0166   | 0.017    | 0.0171   | 0.0166   | 2           |\n",
      "+--------------+---------+---------+---------+---------+---------+---------+---------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-------------+\n",
      "| DBSCAN       | X       | X       | X       | X       | X       | X       | X       | X       | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | X        | --          |\n",
      "+--------------+---------+---------+---------+---------+---------+---------+---------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_results_table(results, optimal_k):\n",
    "    header = [\"Algorithm\"]\n",
    "    k_values = sorted(list(results[\"K-means\"].keys()))\n",
    "    header.extend([f\"k = {k}\" for k in k_values])\n",
    "    header.append(\"Optimal k\")\n",
    "    rows = []\n",
    "\n",
    "    for algorithm in results.keys():\n",
    "        if algorithm == \"DBSCAN\":\n",
    "            row = [algorithm]\n",
    "            for k in k_values:\n",
    "                row.append(\"X\")\n",
    "            row.append(\"--\")\n",
    "        else:\n",
    "            row = [algorithm]\n",
    "            for k in k_values:\n",
    "                silhouette_score = results[algorithm][k][0]\n",
    "                row.append(round(silhouette_score, 4))\n",
    "            row.append(optimal_k[algorithm][\"k\"])\n",
    "        rows.append(row)\n",
    "\n",
    "    table = tabulate(rows, headers=header, tablefmt=\"grid\")\n",
    "    print(table)\n",
    "\n",
    "\n",
    "print_results_table(results, optimal_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c20fd69",
   "metadata": {},
   "source": [
    "#### How did you approach finding the optimal k?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c54da",
   "metadata": {},
   "source": [
    "    For a range of k values (in this case, from 2 to 50), we performed clustering using K-means and Hierarchical clustering algorithms.\n",
    "\n",
    "    For each k value, we computed three evaluation metrics: Silhouette score, Calinski-Harabasz index, and Davies-Bouldin index. These metrics help us understand the quality of the clustering results.\n",
    "\n",
    "        Silhouette score: A higher Silhouette score indicates that the clusters are well-separated and the points within a cluster are close to each other.\n",
    "\n",
    "        Calinski-Harabasz index: A higher Calinski-Harabasz index suggests that the clusters are dense and well-separated.\n",
    "\n",
    "        Davies-Bouldin index: A lower Davies-Bouldin index implies that the clusters are well-separated and compact.\n",
    "\n",
    "    We then looked for the k value that yielded the best results in terms of the evaluation metrics mentioned above. In our approach, we considered the k value that maximized the Silhouette score and Calinski-Harabasz index and minimized the Davies-Bouldin index as the optimal k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ec635",
   "metadata": {},
   "source": [
    "#### What algorithm do you believe is the best? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3136b0a4",
   "metadata": {},
   "source": [
    "it seems that both K-means and Hierarchical clustering methods have the same optimal k value of 2. To determine which model is best, we can look at the silhouette scores for k=2 for both algorithms.\n",
    "\n",
    "K-means Silhouette score for k=2: 0.0277\n",
    "Hierarchical Silhouette score for k=2: 0.0210\n",
    "\n",
    "Since the silhouette score ranges from -1 to 1, with higher values indicating better cluster separation and cohesion, the K-means clustering algorithm performs slightly better than the Hierarchical clustering algorithm with a silhouette score of 0.0277 compared to 0.0210. So, in this case, the K-means clustering model is preferable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e45a2a3",
   "metadata": {},
   "source": [
    "### Add Cluster ID to output file\n",
    "In your data structure, add the cluster id for each smart city respectively. Show the to append the clusterid code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d05dcae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      city                                           raw text   \n",
      "0               VA Norfolk  City of Norfolk, VA\\n*\\nResponse Proposal to U...  \\\n",
      "1            KY Louisville  IMAGINE LOUISVILLE\\n“BEYOND TRAFFIC: THE SMART...   \n",
      "2   MN Minneapolis St Paul  Submitted by:\\nCity of Minneapolis\\nMr. Steven...   \n",
      "3             CA Oceanside   \\n  \\n U.S. Department of Transportation  \\nN...   \n",
      "4                     DC_0   \\n   \\n    \\n \\nSmart  DC \\nMaking  the Distr...   \n",
      "..                     ...                                                ...   \n",
      "59            OH Cleveland   \\n \\n \\n United States  Department of Transpo...   \n",
      "60              MI Detroit  U.S. DEPARTMENT OF TRANSPORTATION - BEYOND TR ...   \n",
      "61           CA San Jose_0  1 \\n Smart City Challenge:  San José on the \\n...   \n",
      "62               CA Fresno  U.S. Department of Transportation \\nNotice of ...   \n",
      "63            AK Anchorage    CONTENTS \\n1 VISION ...........................   \n",
      "\n",
      "                                           clean text  K-means Cluster ID  \n",
      "0   norfolk va response proposal usdot beyond traf...                   1  \n",
      "1   imagine louisville beyond traffic challenge ap...                   1  \n",
      "2   submitted minneapolis steven kotke pe director...                   1  \n",
      "3   department transportation notice funding oppor...                   1  \n",
      "4   dc making district submitted response departme...                   1  \n",
      "..                                                ...                 ...  \n",
      "59  united state department transportation federal...                   1  \n",
      "60  department transportation beyond tr affic chal...                   1  \n",
      "61  challenge san josé move san josé vision san jo...                   1  \n",
      "62  department transportation notice funding oppor...                   1  \n",
      "63  content vision population characteristic site ...                   1  \n",
      "\n",
      "[64 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# First, use the optimal_k values to refit the K-means and Hierarchical models\n",
    "kmeans_optimal = KMeans(n_clusters=optimal_k[\"K-means\"][\"k\"], random_state=42)\n",
    "kmeans_optimal_labels = kmeans_optimal.fit_predict(X)\n",
    "\n",
    "# hierarchical_optimal = AgglomerativeClustering(n_clusters=optimal_k[\"Hierarchical\"][\"k\"])\n",
    "# hierarchical_optimal_labels = hierarchical_optimal.fit_predict(X_dense)\n",
    "\n",
    "# Add the cluster labels to the DataFrame\n",
    "df[\"K-means Cluster ID\"] = kmeans_optimal_labels\n",
    "# df[\"Hierarchical Cluster ID\"] = hierarchical_optimal_labels\n",
    "\n",
    "\n",
    "# If you want to display the DataFrame with cluster IDs, uncomment the following line:\n",
    "print(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "959e7275",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "After finding the best model, it is desirable to have a way to persist the model for future use without having to retrain. Save the model using [model persistance](https://scikit-learn.org/stable/model_persistence.html). This model should be saved in the same directory as this notebook and should be loaded as the model for your `project3.py`.\n",
    "\n",
    "Save the model as `model.pkl`. You do not have to use pickle, but be sure to save the persistance using one of the methods listed in the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c80938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "\n",
    "# Save the model to a file called 'model.pkl'\n",
    "joblib.dump(kmeans, 'model.pkl')\n",
    "\n",
    "# Load the K-means model from the 'model.pkl' file\n",
    "loaded_kmeans = joblib.load('model.pkl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fe5a0c9",
   "metadata": {},
   "source": [
    "## Derving Themes and Concepts\n",
    "\n",
    "Perform Topic Modeling on the cleaned data. Provide the top five words for `TOPIC_NUM = Best_k` as defined in the section above. Feel free to reference [Chapter 6](https://github.com/dipanjanS/text-analytics-with-python/tree/master/New-Second-Edition/Ch06%20-%20Text%20Summarization%20and%20Topic%20Models) for more information on Topic Modeling and Summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75ce0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: pky river mill hill myonkers\n",
      "Topic #2: data transportation system vehicle transit\n"
     ]
    }
   ],
   "source": [
    "# Extract the optimal k value for K-means\n",
    "optimal_k_value = optimal_k['K-means']['k']\n",
    "\n",
    "# Fit the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=optimal_k_value, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "# Function to print top words for each topic\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"Topic #{topic_idx + 1}: \"\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "\n",
    "# Print the top 5 words for each topic\n",
    "print_top_words(lda, vectorizer.get_feature_names_out(), 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573fe65",
   "metadata": {},
   "source": [
    "### Extract themes\n",
    "Write a theme for each topic (atleast a sentence each)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9737d8df",
   "metadata": {},
   "source": [
    "would district atlanta resident city ng metro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21cf24c6",
   "metadata": {},
   "source": [
    "application communication city would downtown operation platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab7df0",
   "metadata": {},
   "source": [
    "### Add Topid ID to output file\n",
    "Add the top two topics for each smart city to the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e937841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      city                                           raw text   \n",
      "0               VA Norfolk  City of Norfolk, VA\\n*\\nResponse Proposal to U...  \\\n",
      "1            KY Louisville  IMAGINE LOUISVILLE\\n“BEYOND TRAFFIC: THE SMART...   \n",
      "2   MN Minneapolis St Paul  Submitted by:\\nCity of Minneapolis\\nMr. Steven...   \n",
      "3             CA Oceanside   \\n  \\n U.S. Department of Transportation  \\nN...   \n",
      "4                     DC_0   \\n   \\n    \\n \\nSmart  DC \\nMaking  the Distr...   \n",
      "..                     ...                                                ...   \n",
      "59            OH Cleveland   \\n \\n \\n United States  Department of Transpo...   \n",
      "60              MI Detroit  U.S. DEPARTMENT OF TRANSPORTATION - BEYOND TR ...   \n",
      "61           CA San Jose_0  1 \\n Smart City Challenge:  San José on the \\n...   \n",
      "62               CA Fresno  U.S. Department of Transportation \\nNotice of ...   \n",
      "63            AK Anchorage    CONTENTS \\n1 VISION ...........................   \n",
      "\n",
      "                                           clean text  K-means Cluster ID   \n",
      "0   norfolk va response proposal usdot beyond traf...                   1  \\\n",
      "1   imagine louisville beyond traffic challenge ap...                   1   \n",
      "2   submitted minneapolis steven kotke pe director...                   1   \n",
      "3   department transportation notice funding oppor...                   1   \n",
      "4   dc making district submitted response departme...                   1   \n",
      "..                                                ...                 ...   \n",
      "59  united state department transportation federal...                   1   \n",
      "60  department transportation beyond tr affic chal...                   1   \n",
      "61  challenge san josé move san josé vision san jo...                   1   \n",
      "62  department transportation notice funding oppor...                   1   \n",
      "63  content vision population characteristic site ...                   1   \n",
      "\n",
      "   Top Topics  \n",
      "0      (1, 0)  \n",
      "1      (1, 0)  \n",
      "2      (1, 0)  \n",
      "3      (1, 0)  \n",
      "4      (1, 0)  \n",
      "..        ...  \n",
      "59     (1, 0)  \n",
      "60     (1, 0)  \n",
      "61     (1, 0)  \n",
      "62     (1, 0)  \n",
      "63     (1, 0)  \n",
      "\n",
      "[64 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Obtain the topic distribution for each city\n",
    "topic_distribution = lda.transform(X)\n",
    "\n",
    "# Find the top two topics for each city\n",
    "top_two_topics = np.argsort(topic_distribution, axis=1)[:, -2:]\n",
    "\n",
    "# Combine the top two topics into a tuple\n",
    "combined_topics = list(zip(top_two_topics[:, 1], top_two_topics[:, 0]))\n",
    "\n",
    "# Add the combined top two topics to the DataFrame\n",
    "df[\"Top Topics\"] = combined_topics\n",
    "\n",
    "# Print the DataFrame with combined top two topics\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39f9c240",
   "metadata": {},
   "source": [
    "## Gathering Applicant Summaries and Keywords\n",
    "\n",
    "For each smart city applicant, gather a summary and keywords that are important to that document. You can use gensim to do this. Here are examples of functions that you could use.\n",
    "\n",
    "```python\n",
    "\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "def summary(text, ratio=0.2, word_count=250, split=False):\n",
    "    return summarize(text, ratio= ratio, word_count=word_count, split=split)\n",
    "    \n",
    "from gensim.summarization import keywords\n",
    "\n",
    "def keys(text, ratio=0.01):\n",
    "    return keywords(text, ratio=ratio)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d27ce37",
   "metadata": {},
   "source": [
    "### Add Summaries and Keywords\n",
    "Add summary and keywords to output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8887d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b328d4a",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f9cb2c4",
   "metadata": {},
   "source": [
    "## Write output data\n",
    "\n",
    "The output data should be written as a TSV file.\n",
    "You can use `to_csv` method from Pandas for this if you are using a DataFrame.\n",
    "\n",
    "`Syntax: df.to_csv('file.tsv', sep = '')` \\\n",
    "`df.to_csv('smartcity_eda.tsv', sep='\\t')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58827464",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('smartcity_eda.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18b8ff",
   "metadata": {},
   "source": [
    "# Moving Forward\n",
    "Now that you have explored the dataset, take the important features and functions to create your `project3.py`.\n",
    "Please refer to the project spec for more guidance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6675ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
